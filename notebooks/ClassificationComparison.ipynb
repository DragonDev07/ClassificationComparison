{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5205a4",
   "metadata": {},
   "source": [
    "# Classification Comparison Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ccfcc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384be7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/.pyenv/versions/AR/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from datasets import load_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e925d7",
   "metadata": {},
   "source": [
    "## Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020c81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# `load_mnist(test_size=0.2, val_size=0.2, random_state=42) -->\n",
    "#   - Loads the mnist dataset and returns it as a dictionary with keys 'train', 'val', and 'test' splits.\n",
    "def load_mnist(test_size=0.2, val_size=0.2, random_state=42):\n",
    "    mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "    X = (\n",
    "        mnist[\"data\"].to_numpy()\n",
    "        if hasattr(mnist[\"data\"], \"to_numpy\")\n",
    "        else mnist[\"data\"]\n",
    "    )\n",
    "    y = (\n",
    "        mnist[\"target\"].astype(int).to_numpy()\n",
    "        if hasattr(mnist[\"target\"], \"to_numpy\")\n",
    "        else mnist[\"target\"].astype(int)\n",
    "    )\n",
    "\n",
    "    # Analyze dataset properties\n",
    "    num_samples = X.shape[0]\n",
    "    pixels = X.shape[1]\n",
    "    side_length = int(np.sqrt(pixels))\n",
    "    input_shape = (1, side_length, side_length)  # (channels, height, width)\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    # Split into train+val and test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Split train+val into train and val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": (X_train, y_train),\n",
    "        \"val\": (X_val, y_val),\n",
    "        \"test\": (X_test, y_test),\n",
    "        \"metadata\": {\n",
    "            \"input_shape\": input_shape,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"dataset_name\": \"MNIST\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# `load_tiny_imagenet(test_size=0.2, val_size=0.2, random_state=42) -->\n",
    "#   - Loads the tiny-imagenet dataset and returns it as a dictionary with keys 'train', 'val', and 'test' splits.\n",
    "def load_tiny_imagenet(test_size=0.2, val_size=0.2, random_state=42):\n",
    "    dataset = load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "\n",
    "    def process_image(img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return np.array(img)\n",
    "\n",
    "    X_full = np.array([process_image(img[\"image\"]) for img in dataset[\"train\"]])\n",
    "    y_full = np.array([img[\"label\"] for img in dataset[\"train\"]])\n",
    "\n",
    "    # Analyze dataset properties\n",
    "    num_samples = X_full.shape[0]\n",
    "    input_shape = (3, X_full.shape[1], X_full.shape[2])  # (channels, height, width)\n",
    "    num_classes = len(np.unique(y_full))\n",
    "\n",
    "    # Reshape images to be flat\n",
    "    X_full_flat = X_full.reshape(X_full.shape[0], -1)\n",
    "\n",
    "    # Split into train+val and test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_full_flat, y_full, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Split train+val into train and val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": (X_train, y_train),\n",
    "        \"val\": (X_val, y_val),\n",
    "        \"test\": (X_test, y_test),\n",
    "        \"metadata\": {\n",
    "            \"input_shape\": input_shape,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"num_samples\": num_samples,\n",
    "            \"dataset_name\": \"tiny-imagenet\",\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae55f5f",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420ed9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Layer 1: First Convolutional Layer\n",
    "        # Output channels: 32\n",
    "        # Kernel size: 3x3\n",
    "        # Stride: 1\n",
    "        # Padding: 1\n",
    "        # Output size: (28-3+1)x(28-3+1) = 26x26\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, 3, 1, padding=1)\n",
    "\n",
    "        # Layer 2: Second Convolutional Layer\n",
    "        # Input channels: 32 (from previous layer)\n",
    "        # Output channels: 64\n",
    "        # Kernel size: 3x3\n",
    "        # Stride: 1\n",
    "        # Padding: 1\n",
    "        # Output size: (26-3+1)x(26-3+1) = 24x24\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Dropout Layers\n",
    "        # Layer 3: First Dropout Layer (25% dropout)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # Layer 5: Second Dropout Layer (50% dropout)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Layer 4: First Fully Connected Layer\n",
    "        # Size will be calculated in forward pass\n",
    "        self.fc1 = None\n",
    "\n",
    "        # Layer 6: Second Fully Connected Layer (Output Layer)\n",
    "        # Input size: 128 neurons\n",
    "        # Output size: Number of Classes\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = self.conv1(x)  # Apply first conv layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        x = self.conv2(x)  # Apply second conv layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        x = F.max_pool2d(x, 2)  # Apply max pooling with 2x2 kernel\n",
    "\n",
    "        # First Dropout\n",
    "        x = self.dropout1(x)  # Apply 25% dropout\n",
    "\n",
    "        # Flatten Layer\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "\n",
    "        # Initialize fc1 if not done yet (to handle different input sizes)\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.shape[1], 128).to(x.device)\n",
    "\n",
    "        # First Fully Connected Layer\n",
    "        x = self.fc1(x)  # Apply first FC layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "\n",
    "        # Second Dropout\n",
    "        x = self.dropout2(x)  # Apply 50% dropout\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.fc2(x)  # Apply second FC layer\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, epochs=10, batch_size=64):\n",
    "        self.model = None\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dataset_name = \"Unknown\"\n",
    "        self.base_path = \"../out\"\n",
    "        self.input_shape = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    # `_generate_paths()` -->\n",
    "    #   - Generate the paths for saving models and visualizations\n",
    "    def _generate_paths(self):\n",
    "        # Create the base directories if they don't exist\n",
    "        model_dir = f\"{self.base_path}/trained_models\"\n",
    "        viz_dir = f\"{self.base_path}/visualizations\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "        # Generate the base name for files\n",
    "        base_name = f\"CNN_{self.epochs}epochs_{self.dataset_name}\"\n",
    "\n",
    "        # Generate complete paths\n",
    "        model_path = f\"{model_dir}/{base_name}.joblib\"\n",
    "        umap_path = f\"{viz_dir}/{base_name}_umap.png\"\n",
    "\n",
    "        return model_path, umap_path\n",
    "\n",
    "    # `_preprocess_data(X)` -->\n",
    "    #   - Preprocess data for training\n",
    "    def _preprocess_data(self, X):\n",
    "        # If data is flat (like MNIST), reshape appropriately\n",
    "        if len(X.shape) == 2:  # Flat data\n",
    "            channels, height, width = self.input_shape\n",
    "            X = X.reshape(-1, channels, height, width)\n",
    "        else:  # Already in image format (N, H, W, C) like TinyImageNet\n",
    "            X = X.reshape(-1, *self.input_shape)\n",
    "            if len(X.shape) == 4:\n",
    "                X = X.transpose(0, 3, 1, 2)  # Convert from (N, H, W, C) to (N, C, H, W)\n",
    "\n",
    "        # Normalize pixel values to [0,1]\n",
    "        X = X.astype(np.float32) / 255.0\n",
    "\n",
    "        return X\n",
    "\n",
    "    # `train(X_train, y_train, X_val, y_val, dataset_name='Unknown')` -->\n",
    "    #   - Train the CNN model\n",
    "    def train(self, X_train, y_train, X_val, y_val, metadata):\n",
    "        # Set metadata from dataset\n",
    "        self.dataset_name = metadata[\"dataset_name\"]\n",
    "        self.input_shape = metadata[\"input_shape\"]\n",
    "        self.num_classes = metadata[\"num_classes\"]\n",
    "\n",
    "        # Initialize model with correct parameters\n",
    "        self.model = CNNModel(\n",
    "            in_channels=self.input_shape[0], num_classes=self.num_classes\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Preprocess data\n",
    "        X_train = self._preprocess_data(X_train)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.LongTensor(y_train.astype(int))\n",
    "\n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(pbar):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                pbar.set_postfix({\"loss\": f\"{train_loss / (batch_idx + 1):.4f}\"})\n",
    "\n",
    "        return self\n",
    "\n",
    "    # `evaluate(X_test, y_test)` -->\n",
    "    #   - Evaluate the model accuracy and prediction speed\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        accuracy = self._evaluate_accuracy(X_test, y_test)\n",
    "        prediction_speed = self._measure_prediction_speed(X_test)\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"prediction_speed\": prediction_speed}\n",
    "\n",
    "    # `predict(X)` -->\n",
    "    #   - Make predictions using the trained model\n",
    "    def predict(self, X, batch_size=16):\n",
    "        self.model.eval()\n",
    "        X = self._preprocess_data(X)\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X).argmax(dim=1).cpu().numpy()\n",
    "        return predictions\n",
    "\n",
    "    # `generate_umap(X, predictions)` -->\n",
    "    #   - Generate and save UMAP visualization\n",
    "    def generate_umap(self, X, predictions):\n",
    "        _, umap_path = self._generate_paths()\n",
    "\n",
    "        # Reshape the array\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, n_jobs=-1)\n",
    "        X_umap = reducer.fit_transform(X_flat)\n",
    "\n",
    "        plt.figure(figsize=(12, 10), dpi=300)\n",
    "        scatter = sns.scatterplot(\n",
    "            x=X_umap[:, 0],\n",
    "            y=X_umap[:, 1],\n",
    "            hue=predictions,\n",
    "            palette=\"tab10\",\n",
    "            alpha=0.8,\n",
    "            s=100,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            legend=\"full\",\n",
    "        )\n",
    "\n",
    "        scatter.legend(title=\"Class\", fontsize=12)\n",
    "        plt.title(\"UMAP Visualization of Predictions\", fontsize=14, pad=20)\n",
    "        plt.xlabel(\"UMAP Component 1\", fontsize=12)\n",
    "        plt.ylabel(\"UMAP Component 2\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(umap_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    # `save()` -->\n",
    "    #   - Save the model to file\n",
    "    def save(self):\n",
    "        model_path, _ = self._generate_paths()\n",
    "        # Save both model and configuration\n",
    "        config = {\n",
    "            \"model\": self.model,\n",
    "            \"input_shape\": self.input_shape,\n",
    "            \"num_classes\": self.num_classes,\n",
    "        }\n",
    "        joblib.dump(config, model_path)\n",
    "        return model_path\n",
    "\n",
    "    # `load()` -->\n",
    "    #   - Load the model from file\n",
    "    def load(self, filename=None):\n",
    "        if filename is None:\n",
    "            model_path, _ = self._generate_paths()\n",
    "            filename = model_path\n",
    "\n",
    "        # Load configuration\n",
    "        config = joblib.load(filename)\n",
    "\n",
    "        # Set model and configuration\n",
    "        self.model = config[\"model\"]\n",
    "        self.input_shape = config[\"input_shape\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "        print(\"Loaded model configuration:\")\n",
    "        print(f\"Input shape: {self.input_shape}\")\n",
    "        print(f\"Number of classes: {self.num_classes}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    # `_evaluate_accuracy(X_test, y_test)` -->\n",
    "    #   - Helper method to evaluate model accuracy\n",
    "    def _evaluate_accuracy(self, X_test, y_test):\n",
    "        self.model.eval()\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_test = self._preprocess_data(X_test)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "        y_test = torch.LongTensor(y_test)\n",
    "\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(test_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        return correct / total\n",
    "\n",
    "    # `_measure_prediction_speed(X_test, n_trials=100)` -->\n",
    "    #   - Helper method to measure prediction speed\n",
    "    def _measure_prediction_speed(self, X_test, n_trials=100):\n",
    "        self.model.eval()\n",
    "\n",
    "        # Preprocess the data first\n",
    "        X_test = self._preprocess_data(X_test)\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "        total_time = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(n_trials)):\n",
    "                start_time = time.time()\n",
    "                self.model(X_test.to(self.device))\n",
    "                total_time += time.time() - start_time\n",
    "\n",
    "        return total_time / n_trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ff722",
   "metadata": {},
   "source": [
    "## Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4b08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "\n",
    "\n",
    "class ClassificationTree:\n",
    "    def __init__(self, base_max_depth=None, random_state=42):\n",
    "        self.base_max_depth = base_max_depth\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.dataset_name = \"Unknown\"\n",
    "        self.is_grid_search = False\n",
    "        self.base_path = \"../out\"\n",
    "\n",
    "    # `_generate_paths()` -->\n",
    "    #   - Generate the paths for saving models and visualizations\n",
    "    def _generate_paths(self):\n",
    "        # Create the base directories if they don't exist\n",
    "        model_dir = f\"{self.base_path}/trained_models\"\n",
    "        viz_dir = f\"{self.base_path}/visualizations\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "        # Generate the base name for files\n",
    "        base_name = f\"ClassificationTree{'_GridSearch' if self.is_grid_search else ''}_{self.dataset_name}\"\n",
    "\n",
    "        # Generate complete paths\n",
    "        model_path = f\"{model_dir}/{base_name}.joblib\"\n",
    "        umap_path = f\"{viz_dir}/{base_name}_umap.png\"\n",
    "\n",
    "        return model_path, umap_path\n",
    "\n",
    "    # `train(X_train, y_train) -->\n",
    "    #   - Directly train the model without grid search\n",
    "    def train(self, X_train, y_train):\n",
    "        self.is_grid_search = False\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = DecisionTreeClassifier(\n",
    "            max_depth=self.base_max_depth, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        # Return self\n",
    "        return self\n",
    "\n",
    "    # `grid_train(X_train, y_train) -->\n",
    "    #   - Train the model using grid search\n",
    "    def grid_train(self, X_train, y_train):\n",
    "        self.is_grid_search = True\n",
    "\n",
    "        # Define the parameter grid\n",
    "        param_grid = {\n",
    "            \"max_depth\": [2, 4, 8, 16, 32, None],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "        }\n",
    "\n",
    "        # Initialize the base classifier\n",
    "        base_classifier = DecisionTreeClassifier(\n",
    "            max_depth=self.base_max_depth, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # Initialize the grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_classifier,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=3,\n",
    "        )\n",
    "\n",
    "        # Fit the grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best model\n",
    "        self.model = grid_search.best_estimator_\n",
    "\n",
    "        # Return self\n",
    "        return self\n",
    "\n",
    "    # `prune(X_val, y_val) -->\n",
    "    #   - Prune the model using cost complexity pruning\n",
    "    def prune(self, X_val, y_val):\n",
    "        # Get the pruning path\n",
    "        pruning_path = self.model.cost_complexity_pruning_path(X_val, y_val)\n",
    "\n",
    "        # Get the alphas and impurities (unused)\n",
    "        alphas, _ = pruning_path.ccp_alphas, pruning_path.impurities\n",
    "\n",
    "        # Initialize variables to store the best classifier and its cross-validation score\n",
    "        best_classifier = None\n",
    "        best_cv_mean_score = 0\n",
    "\n",
    "        # Loop through the alphas and prune the model\n",
    "        for alpha in tqdm(alphas):\n",
    "            # Initialize the pruned classifier\n",
    "            pruned_classifier = DecisionTreeClassifier(\n",
    "                random_state=self.random_state, ccp_alpha=alpha\n",
    "            )\n",
    "\n",
    "            # Fit the pruned classifier\n",
    "            pruned_classifier.fit(X_val, y_val)\n",
    "\n",
    "            # Evaluate the pruned classifier\n",
    "            cv_score = pruned_classifier.score(X_val, y_val)\n",
    "\n",
    "            # Check if the current classifier has a better cross-validation score\n",
    "            if cv_score > best_cv_mean_score:\n",
    "                # Update the best classifier and its cross-validation score\n",
    "                best_classifier = pruned_classifier\n",
    "                best_cv_mean_score = cv_score\n",
    "\n",
    "        # Return self\n",
    "        self.model = best_classifier\n",
    "        return self\n",
    "\n",
    "    # `evaluate(X_test, y_test, cv=5) -->\n",
    "    #   - Evaluate the model using:\n",
    "    #       - Cross-validation score\n",
    "    #       - Accuracy score\n",
    "    #       - Prediction speed\n",
    "    def evaluate(self, X_test, y_test, cv=5):\n",
    "        cv_score = cross_val_score(self.model, X_test, y_test, cv=cv).mean()\n",
    "        predictions = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        prediction_speed = self._measure_prediction_speed(X_test)\n",
    "\n",
    "        return {\n",
    "            \"cv_score\": cv_score,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"prediction_speed\": prediction_speed,\n",
    "        }\n",
    "\n",
    "    # `predict(X)` -->\n",
    "    #   - Predict the class labels for the given data\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    # `generate_umap(X, predictions, save_path=None)` -->\n",
    "    #   - Generate UMAP visualization\n",
    "    def generate_umap(self, X, predictions):\n",
    "        # Get save path\n",
    "        _, umap_path = self._generate_paths()\n",
    "\n",
    "        # Initialize UMAP reducer\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, n_jobs=-1)\n",
    "        X_umap = reducer.fit_transform(X)\n",
    "\n",
    "        # Initialize plot\n",
    "        plt.figure(figsize=(12, 10), dpi=300)\n",
    "        scatter = sns.scatterplot(\n",
    "            x=X_umap[:, 0],\n",
    "            y=X_umap[:, 1],\n",
    "            hue=predictions,\n",
    "            palette=\"tab10\",\n",
    "            alpha=0.8,\n",
    "            s=100,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            legend=\"full\",\n",
    "        )\n",
    "\n",
    "        # Initialize legend\n",
    "        scatter.legend(title=\"Class\", fontsize=12)\n",
    "        plt.title(\"UMAP Visualization of Predictions\", fontsize=14, pad=20)\n",
    "        plt.xlabel(\"UMAP Component 1\", fontsize=12)\n",
    "        plt.ylabel(\"UMAP Component 2\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if umap_path:\n",
    "            # Save plot\n",
    "            plt.savefig(umap_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    # `save(self, filename)` -->\n",
    "    #    - Save the model to a file\n",
    "    def save(self):\n",
    "        # Generate model path\n",
    "        model_path, _ = self._generate_paths()\n",
    "\n",
    "        # Save the model to a file\n",
    "        joblib.dump(self.model, model_path)\n",
    "        return model_path\n",
    "\n",
    "    # `load(self, filename)` -->\n",
    "    #    - Load the model from a file\n",
    "    def load(self, filename=None):\n",
    "        if filename is None:\n",
    "            # Generate paths if filename is not provided\n",
    "            model_path, *_ = self._generate_paths()\n",
    "            filename = model_path\n",
    "\n",
    "        # Load the model from the file\n",
    "        self.model = joblib.load(filename)\n",
    "        return self\n",
    "\n",
    "    # `_measure_prediction_speed(self, X_test, n_trials=100)` -->\n",
    "    #    - Helper function to measure average prediction speed\n",
    "    def _measure_prediction_speed(self, X_test, n_trials=100):\n",
    "        total_time = 0\n",
    "        for _ in tqdm(range(n_trials)):\n",
    "            start_time = time.time()\n",
    "            self.model.predict(X_test)\n",
    "            total_time += time.time() - start_time\n",
    "        return total_time / n_trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773fad6",
   "metadata": {},
   "source": [
    "## Main Execution (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    print(\"<< === Loading Data === >>\")\n",
    "    # print(\"--> Loading tiny-imagenet\")\n",
    "    # data = load_tiny_imagenet()\n",
    "    print(\"--> Loading MNIST\")\n",
    "    data = load_mnist()\n",
    "\n",
    "    # Separate data into sets\n",
    "    X_train, y_train = data[\"train\"]\n",
    "    X_val, y_val = data[\"val\"]\n",
    "    X_test, y_test = data[\"test\"]\n",
    "    metadata = data[\"metadata\"]\n",
    "\n",
    "    # ---------- Classification Tree ---------- #\n",
    "    print(\"<< === Classification Tree === >>\")\n",
    "    clf_tree = ClassificationTree(base_max_depth=3)\n",
    "    clf_tree.dataset_name = \"MNIST\"\n",
    "\n",
    "    print(\"--> Loading Model\")\n",
    "    clf_tree.load()\n",
    "\n",
    "    print(\"--> Training Model\")\n",
    "    clf_tree.train(X_train, y_train)\n",
    "\n",
    "    print(\"--> Pruning Model\")\n",
    "    clf_tree.prune(X_val, y_val)\n",
    "\n",
    "    print(\"--> Saving Model\")\n",
    "    clf_tree.save()\n",
    "\n",
    "    print(\"--> Running Predictions\")\n",
    "    predictions = clf_tree.predict(X_test)\n",
    "\n",
    "    print(\"--> Generating UMAP\")\n",
    "    clf_tree.generate_umap(X_test, predictions)\n",
    "\n",
    "    print(\"--> Evaluating Model\")\n",
    "    clf_tree_evaluation = clf_tree.evaluate(X_test, y_test)\n",
    "\n",
    "    print(\"\\n----- RESULTS -----\")\n",
    "    print(f\"Accuracy: {clf_tree_evaluation['accuracy']}\")\n",
    "\n",
    "    # --------- CNN Deep Learning ---------- #\n",
    "    print(\"<< === CNN Deep Learning === >>\")\n",
    "    cnn = CNN(epochs=20, batch_size=64)\n",
    "    cnn.dataset_name = \"MNIST\"\n",
    "\n",
    "    # print(\"--> Loading Model\")\n",
    "    # cnn.load()\n",
    "\n",
    "    print(\"--> Training Model\")\n",
    "    cnn.train(X_train, y_train, X_val, y_val, metadata)\n",
    "\n",
    "    print(\"--> Saving Model\")\n",
    "    cnn.save()\n",
    "\n",
    "    print(\"--> Running Predictions\")\n",
    "    cnn_predictions = cnn.predict(X_test)\n",
    "\n",
    "    print(\"--> Generating UMAP\")\n",
    "    cnn.generate_umap(X_test, cnn_predictions)\n",
    "\n",
    "    print(\"--> Evaluating Model\")\n",
    "    cnn_evaluation = cnn.evaluate(X_test, y_test)\n",
    "\n",
    "    print(\"\\n----- RESULTS -----\")\n",
    "    print(f\"Accuracy: {cnn_evaluation['accuracy']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c7652",
   "metadata": {},
   "source": [
    "## Main Execution (tiny-imagenet -> ISSUE HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1ecd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< === Loading Data === >>\n",
      "--> Loading tiny-imagenet\n",
      "<< === Classification Tree === >>\n",
      "--> Training Model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    print(\"<< === Loading Data === >>\")\n",
    "    print(\"--> Loading tiny-imagenet\")\n",
    "    data = load_tiny_imagenet()\n",
    "\n",
    "    # Separate data into sets\n",
    "    X_train, y_train = data[\"train\"]\n",
    "    X_val, y_val = data[\"val\"]\n",
    "    X_test, y_test = data[\"test\"]\n",
    "    metadata = data[\"metadata\"]\n",
    "\n",
    "    # ---------- Classification Tree ---------- #\n",
    "    print(\"<< === Classification Tree === >>\")\n",
    "    clf_tree = ClassificationTree(base_max_depth=3)\n",
    "    clf_tree.dataset_name = \"tiny-imagenet\"\n",
    "\n",
    "    # print(\"--> Loading Model\")\n",
    "    # clf_tree.load()\n",
    "\n",
    "    print(\"--> Training Model\")\n",
    "    clf_tree.train(X_train, y_train)\n",
    "\n",
    "    print(\"--> Pruning Model\")\n",
    "    clf_tree.prune(X_val, y_val)\n",
    "\n",
    "    print(\"--> Saving Model\")\n",
    "    clf_tree.save()\n",
    "\n",
    "    print(\"--> Running Predictions\")\n",
    "    predictions = clf_tree.predict(X_test)\n",
    "\n",
    "    print(\"--> Generating UMAP\")\n",
    "    clf_tree.generate_umap(X_test, predictions)\n",
    "\n",
    "    print(\"--> Evaluating Model\")\n",
    "    clf_tree_evaluation = clf_tree.evaluate(X_test, y_test)\n",
    "\n",
    "    print(\"\\n----- RESULTS -----\")\n",
    "    print(f\"Accuracy: {clf_tree_evaluation['accuracy']}\")\n",
    "\n",
    "    # --------- CNN Deep Learning ---------- #\n",
    "    print(\"<< === CNN Deep Learning === >>\")\n",
    "    cnn = CNN(epochs=20, batch_size=64)\n",
    "    cnn.dataset_name = \"tiny-imagenet\"\n",
    "\n",
    "    # print(\"--> Loading Model\")\n",
    "    # cnn.load()\n",
    "\n",
    "    print(\"--> Training Model\")\n",
    "    cnn.train(X_train, y_train, X_val, y_val, metadata)\n",
    "\n",
    "    print(\"--> Saving Model\")\n",
    "    cnn.save()\n",
    "\n",
    "    print(\"--> Running Predictions\")\n",
    "    cnn_predictions = cnn.predict(X_test)\n",
    "\n",
    "    print(\"--> Generating UMAP\")\n",
    "    cnn.generate_umap(X_test, cnn_predictions)\n",
    "\n",
    "    print(\"--> Evaluating Model\")\n",
    "    cnn_evaluation = cnn.evaluate(X_test, y_test)\n",
    "\n",
    "    print(\"\\n----- RESULTS -----\")\n",
    "    print(f\"Accuracy: {cnn_evaluation['accuracy']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7382c3-c866-4d17-98ab-7e25fdacd1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AR)",
   "language": "python",
   "name": "ar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
